{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wanted-preOnboarding_submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 1"
      ],
      "metadata": {
        "id": "Vg9WmnrmcQRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from math import log"
      ],
      "metadata": {
        "id": "VsMBWTe3sNB1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "eCqCsWfgbA04"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "\n",
        "    # print(\"*********************start Tokenizer preprocessing*********************\")\n",
        "    '''\n",
        "    문제 1-1.\n",
        "    '''\n",
        "    for sequence in sequences:\n",
        "\n",
        "      #조건 1. 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
        "      sequence = sequence.lower()\n",
        "      sequence = re.sub(\"[0-9\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>·@\\#$%&\\\\\\=\\(\\'\\\"]\", \"\", sequence)\n",
        "      #조건 2. 토큰화는 white space 단위로 수행합니다.\n",
        "      tokens = sequence.split()\n",
        "\n",
        "      result.append(tokens)\n",
        "\n",
        "    # print(\"preprocessing result : \", result)\n",
        "    # print(\"*********************end Tokenizer preprocessing*********************\")\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    # print(\"*********************start Tokenizer fit*********************\")\n",
        "    '''\n",
        "    문제 1-2.\n",
        "    '''\n",
        "    #조건 1. 위에서 만든 preprocessing 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
        "    sequence = self.preprocessing(sequences)\n",
        "    #조건 2. 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(self.word_dict)을 생성합니다.\n",
        "    token_set = set()\n",
        "    token_list = []\n",
        "\n",
        "    for tokens in sequence:\n",
        "      tokens_set = set(tokens)\n",
        "      token_set.update(tokens_set)\n",
        "\n",
        "    for tokens in sequence:\n",
        "      for token in tokens:\n",
        "        if token in token_set:\n",
        "          token_list.append(token)\n",
        "          token_set.remove(token)\n",
        "\n",
        "    numbers = list(range(1, len(token_list)+1))\n",
        "\n",
        "    self.word_dict.update(dict(zip(token_list, numbers)))\n",
        "\n",
        "    # print(\"*********************end Tokenizer fit*********************\")\n",
        "\n",
        "    self.fit_checker = True\n",
        "    \n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    # print(\"*********************start Tokenizer transform*********************\")\n",
        "    result = []\n",
        "    token_result = []\n",
        "    sequence = self.preprocessing(sequences)\n",
        "    if self.fit_checker == True:\n",
        "      '''\n",
        "      문제 1-3.\n",
        "      '''\n",
        "      #조건. 어휘 사전(self.word_dict)에 없는 단어는 'oov'의 index로 변환합니다.\n",
        "\n",
        "      for tokens in sequence:\n",
        "        for token in tokens:\n",
        "          if token not in self.word_dict:\n",
        "            token_result.append(self.word_dict['oov'])\n",
        "          \n",
        "          else:\n",
        "            token_result.append(self.word_dict[token])\n",
        "        \n",
        "        result.append(token_result)\n",
        "        token_result = []\n",
        "      # print(\"transform result : \", result )\n",
        "      # print(\"*********************end Tokenizer transform*********************\")\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "    \n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 2"
      ],
      "metadata": {
        "id": "Y4s9h6ebcNEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.idf_matrix = []\n",
        "    self.tfidf_matrix = []\n",
        "    self.fit_checker = False\n",
        "\n",
        "  def tf(self, t, d):\n",
        "    return d.count(t)\n",
        "\n",
        "  def idf(self, sequences, t):\n",
        "    df = 0\n",
        "    N = len(sequences)\n",
        "    for sequence in sequences:\n",
        "      df += t in sequence\n",
        "    return log(N/df+1)\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    # print(\"*********************start TfidfVectorizer fit*********************\")\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    '''\n",
        "    문제 2-1.\n",
        "    '''\n",
        "    #조건 1. IDF 행렬은 list 형태입니다.\n",
        "    #조건 2. IDF 값은 아래 식을 이용해 구합니다.\n",
        "    #조건 3. 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다.\n",
        "\n",
        "    for tokens in tokenized:\n",
        "      for t in tokens:\n",
        "        self.idf_matrix.append(self.idf(tokenized, t))\n",
        "\n",
        "    print(\"idf_matrix\", self.idf_matrix)\n",
        "    self.fit_checker = True\n",
        "    # print(\"*********************end TfidfVectorizer fit*********************\")\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker == True:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      '''\n",
        "      문제 2-2.\n",
        "      '''\n",
        "      #조건 1. 입력 문장을 이용해 TF 행렬을 만드세요.\n",
        "\n",
        "      tf_matrix = []\n",
        "\n",
        "      for tokens in tokenized:\n",
        "        d = tokens\n",
        "        for t in tokens:\n",
        "          tf_matrix.append(self.tf(t, d))\n",
        "\n",
        "      print(\"tf_matrix : \", tf_matrix)\n",
        "\n",
        "      #조건 2. 문제 2-1( fit())에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
        "      for tokens in tokenized:\n",
        "        self.tfidf_matrix.append([])\n",
        "        for i in range(len(tokens)):\n",
        "          self.tfidf_matrix[-1].append(tf_matrix[i] * self.idf_matrix[i])\n",
        "\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "scwzEq-pcLpR"
      },
      "execution_count": 99,
      "outputs": []
    }
  ]
}